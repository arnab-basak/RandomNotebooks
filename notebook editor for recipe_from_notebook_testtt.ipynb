{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataiku\n",
    "from dataiku import pandasutils as pdu\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataiku\n",
    "from dataiku import pandasutils as pdu\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "#Vitrox = dataiku.Folder(\"Vitrox\")\n",
    "#VitroxS = dataiku.Folder(\"Vitrox_split\")\n",
    "#Adice = dataiku.Folder(\"Abnormal_dice\")\n",
    "#segment = dataiku.Folder(\"Segment\")\n",
    "#test = dataiku.Folder(\"test_augmented\")\n",
    "test = dataiku.Folder(\"tpak_axi_test\")\n",
    "#train = dataiku.Folder(\"tpak_axi_train\")\n",
    "#train_aug = dataiku.Folder(\"train_augmented\")\n",
    "\n",
    "#resnet = dataiku.Folder(\"google_resnet_v2\")\n",
    "#resnet_info=resnet.get_info()\n",
    "#resnet_path=resnet_info['path']\n",
    "\n",
    "\n",
    "resnet = dataiku.Folder(\"efficientV2\")\n",
    "resnet_info=resnet.get_info()\n",
    "resnet_path=resnet_info['path']\n",
    "\n",
    "#Vitrox_info=Vitrox.get_info()\n",
    "#VitroxS_info=VitroxS.get_info()\n",
    "#segment_info=segment.get_info()\n",
    "test_info=test.get_info()\n",
    "#train_info=train.get_info()\n",
    "#train_aug_info=train_aug.get_info()\n",
    "\n",
    "#Void = dataiku.Folder(\"Void\")\n",
    "\n",
    "#Vitrox_path=Vitrox_info['path']\n",
    "#VitroxS_path=VitroxS_info['path']\n",
    "#segment_path=segment_info['path']\n",
    "test_path=test_info['path']\n",
    "#train_path=train_info['path']\n",
    "#train_aug_path=train_aug_info['path']\n",
    "\n",
    "# Recipe outputs\n",
    "out = dataiku.Folder(\"57FA1eZU\")\n",
    "out_info = out.get_info()\n",
    "out_path=out_info['path']\n",
    "model_path=out_path+'/effmodel2.h5'\n",
    "cm_out = dataiku.Folder(\"cm_out\")\n",
    "cm_out_info = cm_out.get_info()\n",
    "cm_out_path=cm_out_info['path']\n",
    "path_img=cm_out_path+'/cmeffmodel2.png'\n",
    "#VitroxS_path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "import tqdm as notebook_tqdm\n",
    "import splitfolders\n",
    "import os\n",
    "#import cv2\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.layers import Dense,Flatten,Reshape,Lambda,LeakyReLU,Dropout,\\\n",
    "                                    BatchNormalization,Conv1D,Conv2D,Reshape,MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "#from tabulate import tabulate\n",
    "#from pyDOE import lhs\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.layers import Conv2D,\\\n",
    "    MaxPool2D, Conv2DTranspose, Input, Activation,\\\n",
    "    Concatenate, CenterCrop\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.optimizers import schedules, Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy,KLDivergence\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from skimage import data, color\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from skimage import io\n",
    "from skimage.util import crop\n",
    "\n",
    "\n",
    "import tensorflow.keras.callbacks as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import UpSampling2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import Add\n",
    "from keras.layers import Multiply\n",
    "from keras.layers import Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "    tf.config.set_visible_devices([], 'GPU')  ## only this works ?!!\n",
    "else:\n",
    "\n",
    "    print(\"Cannot detect GPU in tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "img_height = 512\n",
    "img_width = 512\n",
    "def process(image,label):\n",
    "    image = tf.cast(image/255. ,tf.float32)\n",
    "    return image,label\n",
    "'''train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  train_aug_path,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  train_aug_path,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(process)\n",
    "val_ds = val_ds.map(process)\n",
    "\n",
    "for images, labels in train_ds.take(1):\n",
    "    print(images.shape)\n",
    "    print(images[0])\n",
    "\n",
    "for images, labels in val_ds.take(1):\n",
    "    print(images.shape)\n",
    "    print(images[0])'''\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_path\n",
    "\n",
    "test_ds=test_datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(img_height,img_width),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    )\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.Normalization(),\n",
    "        #layers.experimental.preprocessing.Resizing(img_height, img_width),\n",
    "        #layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        #layers.experimental.preprocessing.RandomRotation(factor=0.02),\n",
    "        #layers.experimental.preprocessing.RandomZoom(\n",
    "        #    height_factor = 0.2, width_factor = 0.2\n",
    "        #),\n",
    "        #tf.keras.layers.RandomTranslation(height_factor=0.2, width_factor=0.2),\n",
    "        #tf.keras.layers.RandomContrast(0.2),\n",
    "        #layers.RandomContrast(0.2),\n",
    "        #layers.RandomTranslation(height_factor=0.2, width_factor=0.2),\n",
    "        #tfm.vision.preprocess_ops.color_jitter()\n",
    "    ],\n",
    "     name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "#CompleteBatchData  =next(iter(test_ds))[0]\n",
    "#data_augmentation.layers[0].adapt(CompleteBatchData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNetClassifier(shape=(img_height, img_width, 3), n_channels=64, n_classes=2,\n",
    "                      dropout=0.2, regularization=0.01):\n",
    "    \"\"\"\n",
    "    Attention-56 ResNet\n",
    "    https://arxiv.org/abs/1704.06904\n",
    "    \"\"\"\n",
    "\n",
    "    regularizer = l2(regularization)\n",
    "\n",
    "    input_ = Input(shape=shape)\n",
    "    #x=data_augmentation(input_)\n",
    "    #x = Conv2D(n_channels, (7, 7), strides=(2, 2), padding='same')(input_)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x = Activation('relu')(x)\n",
    "    #x = MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    resnet= tf.keras.Sequential([\n",
    "            hub.KerasLayer(resnet_path,\n",
    "                   trainable=True),  # Can be True, see below.\n",
    "        ]\n",
    "        )(input_)\n",
    "    #output = Dense(1, kernel_regularizer=regularizer, activation='sigmoid')(resnet)\n",
    "    output = Dense(n_classes, kernel_regularizer=regularizer, activation='softmax')(resnet)\n",
    "\n",
    "    model = Model(input_, output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNetClassifier(n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=2, min_lr=10e-7, cooldown=1, verbose=1)\n",
    "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=7, verbose=1)\n",
    "callbacks= [lr_reducer, early_stopper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate=0.00001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(keras.optimizers.Adam(learning_rate=0.00001), loss=tfa.losses.SigmoidFocalCrossEntropy( from_logits= False,\n",
    "#    alpha = 0.25,gamma = 2.0,reduction= tf.keras.losses.Reduction.AUTO,name = 'sigmoid_focal_crossentropy'), metrics=['accuracy'])\n",
    "model.compile(keras.optimizers.Adam(learning_rate=0.00001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history=model.fit(train_ds,batch_size=8,epochs=50,validation_data=val_ds,callbacks=callbacks)\n",
    "#history=model.fit_generator(\n",
    "    #train_ds,\n",
    "    #steps_per_epoch = train_ds.samples // batch_size,\n",
    "    #validation_data = val_ds,\n",
    "    #validation_steps = val_ds.samples // batch_size,\n",
    "    #epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(keras.optimizers.Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path\n",
    "\n",
    "model.build(input_shape =(None,img_height,img_width,3))\n",
    "model.load_weights(model_path)\n",
    "\n",
    "model\n",
    "\n",
    "model.summary()\n",
    "\n",
    "results=model.evaluate(test_ds, batch_size=8)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_steps = test_ds.n // batch_size\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# Iterate over the validation data generator and predict labels in batches\n",
    "for i in range(test_steps):\n",
    "    X_val, y_val = test_ds.next()\n",
    "    #print(y_val)\n",
    "    y_pred_batch = model.predict(X_val)\n",
    "    #print(np.argmax(y_pred_batch, axis=1))\n",
    "    y_pred.extend(np.argmax(y_pred_batch, axis=1))\n",
    "    #print(np.argmax(y_val, axis=1))\n",
    "    y_true.extend(np.argmax(y_val, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = ['Good', 'Defect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "#fig=plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "#plt.title('Confusion Matrix')\n",
    "#plt.colorbar()\n",
    "#tick_marks = np.arange(len(class_labels))\n",
    "#plt.xticks(tick_marks, class_labels)\n",
    "#plt.yticks(tick_marks, class_labels)\n",
    "\n",
    "# Label the matrix cells with the counts\n",
    "#thresh = cm.max() / 2\n",
    "#for i, j in np.ndindex(cm.shape):\n",
    "#    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "#             horizontalalignment=\"center\",\n",
    "#             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "#\n",
    "#plt.ylabel('True Label')\n",
    "#plt.xlabel('Predicted Label')\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig=sns.heatmap(cm,annot=True,fmt='g',linewidth=0.1,linecolor='black',xticklabels=['good','bad'],yticklabels=['good','bad'],\n",
    "            cmap=\"Blues\")\n",
    "plt.ylabel('Prediction',fontsize=13)\n",
    "plt.xlabel('Actual',fontsize=13)\n",
    "plt.title('Confusion Matrix',fontsize=17)\n",
    "plt.show()\n",
    "plt.savefig(path_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_true, y_pred, average='binary')\n",
    "print(\"precision:\",precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_true, y_pred, average='binary')\n",
    "print(\"recall:\",recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=sklearn.metrics.f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 score:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recipe outputs\n",
    "wmm8v6au = dataiku.Folder(\"wMM8v6AU\")\n",
    "wmm8v6au_info = wmm8v6au.get_info()"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "recipe_from_notebook_testtt",
  "createdOn": 1705314470801,
  "creator": "arnab.basak",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (env SentTransform)",
   "language": "python",
   "name": "py-dku-venv-senttransform"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "modifiedBy": "arnab.basak",
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataiku\n",
    "from dataiku import pandasutils as pdu\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataiku\n",
    "from dataiku import pandasutils as pdu\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "#Vitrox = dataiku.Folder(\"Vitrox\")\n",
    "#VitroxS = dataiku.Folder(\"Vitrox_split\")\n",
    "#Adice = dataiku.Folder(\"Abnormal_dice\")\n",
    "#segment = dataiku.Folder(\"Segment\")\n",
    "test = dataiku.Folder(\"tpak_axi_test\")\n",
    "train = dataiku.Folder(\"tpak_axi_train\")\n",
    "train_aug = dataiku.Folder(\"train_augmented\")\n",
    "\n",
    "resnet = dataiku.Folder(\"efficientV2\")\n",
    "resnet_info=resnet.get_info()\n",
    "resnet_path=resnet_info['path']\n",
    "\n",
    "\n",
    "#Vitrox_info=Vitrox.get_info()\n",
    "#VitroxS_info=VitroxS.get_info()\n",
    "#segment_info=segment.get_info()\n",
    "test_info=test.get_info()\n",
    "train_info=train.get_info()\n",
    "train_aug_info=train_aug.get_info()\n",
    "\n",
    "#Void = dataiku.Folder(\"Void\")\n",
    "\n",
    "#Vitrox_path=Vitrox_info['path']\n",
    "#VitroxS_path=VitroxS_info['path']\n",
    "#segment_path=segment_info['path']\n",
    "test_path=test_info['path']\n",
    "train_path=train_info['path']\n",
    "train_aug_path=train_aug_info['path']\n",
    "\n",
    "# Recipe outputs\n",
    "out = dataiku.Folder(\"57FA1eZU\")\n",
    "out_info = out.get_info()\n",
    "out_path=out_info['path']\n",
    "model_path=out_path+'/effmodel3.h5'\n",
    "path_img=out_path+'/effmodel3.png'\n",
    "#VitroxS_path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "import tqdm as notebook_tqdm\n",
    "import splitfolders\n",
    "import os\n",
    "#import cv2\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.layers import Dense,Flatten,Reshape,Lambda,LeakyReLU,Dropout,\\\n",
    "                                    BatchNormalization,Conv1D,Conv2D,Reshape,MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "#from tabulate import tabulate\n",
    "#from pyDOE import lhs\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.layers import Conv2D,\\\n",
    "    MaxPool2D, Conv2DTranspose, Input, Activation,\\\n",
    "    Concatenate, CenterCrop\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.optimizers import schedules, Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy,KLDivergence\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from skimage import data, color\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from skimage import io\n",
    "from skimage.util import crop\n",
    "\n",
    "\n",
    "import tensorflow.keras.callbacks as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import UpSampling2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import Add\n",
    "from keras.layers import Multiply\n",
    "from keras.layers import Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "    tf.config.set_visible_devices([], 'GPU')  ## only this works ?!!\n",
    "else:\n",
    "\n",
    "    print(\"Cannot detect GPU in tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "img_height = 512\n",
    "img_width = 512\n",
    "def process(image,label):\n",
    "    image = tf.cast(image/255. ,tf.float32)\n",
    "    return image,label\n",
    "'''train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  train_aug_path,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  train_aug_path,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(process)\n",
    "val_ds = val_ds.map(process)\n",
    "\n",
    "for images, labels in train_ds.take(1):\n",
    "    print(images.shape)\n",
    "    print(images[0])\n",
    "\n",
    "for images, labels in val_ds.take(1):\n",
    "    print(images.shape)\n",
    "    print(images[0])'''\n",
    "\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    #rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    shear_range=0.2,\n",
    "    #vertical_flip=True,\n",
    "    #horizontal_flip = True,\n",
    "    #zoom_range = [0.8, 1.2],\n",
    "    rescale=1./255,\n",
    "    validation_split=0.1\n",
    "    )\n",
    "train_ds=train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(img_height,img_width),\n",
    "    color_mode='rgb',\n",
    "    class_mode='binary',\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    subset='training'\n",
    "    )\n",
    "val_ds=train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(img_height,img_width),\n",
    "    color_mode='rgb',\n",
    "    class_mode='binary',\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    subset='validation'\n",
    "    )\n",
    "#train_ds_std=train_datagen.standardize(train_ds)\n",
    "#train_datagen.fit(train_ds)\n",
    "#seed = (1, 2)\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.Normalization(),\n",
    "        layers.experimental.preprocessing.Resizing(img_height, img_width),\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(factor=0.02),\n",
    "        layers.experimental.preprocessing.RandomZoom(\n",
    "            height_factor = 0.2, width_factor = 0.2\n",
    "        ),\n",
    "        tf.keras.layers.RandomTranslation(height_factor=0.2, width_factor=0.2),\n",
    "        tf.keras.layers.RandomContrast(0.2),\n",
    "        #layers.RandomContrast(0.2),\n",
    "        #layers.RandomTranslation(height_factor=0.2, width_factor=0.2),\n",
    "        #tfm.vision.preprocess_ops.color_jitter()\n",
    "    ],\n",
    "     name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "CompleteBatchData  =next(iter(train_ds))[0]\n",
    "data_augmentation.layers[0].adapt(CompleteBatchData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNetClassifier(shape=(img_height,img_width, 3), n_channels=64, n_classes=2,\n",
    "                      dropout=0.2, regularization=0.01):\n",
    "    \"\"\"\n",
    "    Attention-56 ResNet\n",
    "    https://arxiv.org/abs/1704.06904\n",
    "    \"\"\"\n",
    "\n",
    "    regularizer = l2(regularization)\n",
    "\n",
    "    input_ = layers.Input(shape=shape)\n",
    "    #x = Conv2D(n_channels, (7, 7), strides=(2, 2), padding='same')(input_)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x = Activation('relu')(x)\n",
    "    #x = MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x=data_augmentation(input_)\n",
    "    #x=tf.image.random_brightness(x,max_delta=0.2),\n",
    "    #x=tf.image.random_contrast(x,0.2, 0.5),\n",
    "    resnet= tf.keras.Sequential([\n",
    "            hub.KerasLayer(resnet_path,\n",
    "                   trainable=True)  # Can be True, see below.\n",
    "        ]\n",
    "        )(x)\n",
    "    #x=Dense(1024,kernel_regularizer=regularizer, activation=LeakyReLU(alpha=0.01))(resnet)\n",
    "    #x=Dropout(0.2)(x)\n",
    "    #x=Dense(512,kernel_regularizer=regularizer, activation=LeakyReLU(alpha=0.01))(x)\n",
    "    #x=Dropout(0.2)(x)\n",
    "    #x=Dense(256,kernel_regularizer=regularizer, activation=LeakyReLU(alpha=0.01))(x)\n",
    "    #x=Dropout(0.2)(x)\n",
    "    #output = Dense(1, kernel_regularizer=regularizer, activation='sigmoid')(resnet)\n",
    "    output = Dense(n_classes, kernel_regularizer=regularizer, activation='softmax')(resnet)\n",
    "\n",
    "    model = Model(input_, output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNetClassifier(n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=2, min_lr=10e-7, cooldown=1, verbose=1)\n",
    "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=7, verbose=1)\n",
    "callbacks= [lr_reducer, early_stopper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate=0.00001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(keras.optimizers.Adam(learning_rate=0.00001), loss=tfa.losses.SigmoidFocalCrossEntropy( from_logits= False,\n",
    "#    alpha = 0.25,gamma = 2.0,reduction= tf.keras.losses.Reduction.AUTO,name = 'sigmoid_focal_crossentropy'), metrics=['accuracy'])\n",
    "model.compile(keras.optimizers.Adam(learning_rate=0.00001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(train_ds,batch_size=8,epochs=50,validation_data=val_ds,callbacks=callbacks)\n",
    "#history=model.fit_generator(\n",
    "    #train_ds,\n",
    "    #steps_per_epoch = train_ds.samples // batch_size,\n",
    "    #validation_data = val_ds,\n",
    "    #validation_steps = val_ds.samples // batch_size,\n",
    "    #epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.evaluate(test_ds, batch_size=8)\n",
    "loss_values = history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "accuracy=history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "plt.plot(loss_values, label='Training Loss')\n",
    "plt.plot(val_loss,label='Validation Loss')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.savefig(path_img)\n",
    "\n",
    "model.save_weights(model_path)"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "recipe_from_notebook_MaskResNet",
  "createdOn": 1705314458805,
  "creator": "arnab.basak",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (env SentTransform)",
   "language": "python",
   "name": "py-dku-venv-senttransform"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "modifiedBy": "arnab.basak",
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
